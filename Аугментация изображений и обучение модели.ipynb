{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksVGvNfokr0T",
        "outputId": "b7f2fbda-9082-4b01-9b84-6938e776abc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-17 13:07:16--  https://storage.yandexcloud.net/academy.ai/cat-and-dog.zip\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228082266 (218M) [application/x-zip-compressed]\n",
            "Saving to: ‘cat-and-dog.zip’\n",
            "\n",
            "cat-and-dog.zip     100%[===================>] 217.52M  16.2MB/s    in 13s     \n",
            "\n",
            "2024-10-17 13:07:31 (16.3 MB/s) - ‘cat-and-dog.zip’ saved [228082266/228082266]\n",
            "\n",
            "Found 6404 images belonging to 2 classes.\n",
            "Found 1601 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 485ms/step - accuracy: 0.9236 - loss: 0.2140 - val_accuracy: 0.9762 - val_loss: 0.0605\n",
            "Epoch 2/10\n",
            "\u001b[1m  1/200\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.8750 - loss: 0.1737"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8750 - loss: 0.1737 - val_accuracy: 1.0000 - val_loss: 1.3828e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 459ms/step - accuracy: 0.9693 - loss: 0.0821 - val_accuracy: 0.9538 - val_loss: 0.1258\n",
            "Epoch 4/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9688 - loss: 0.1677 - val_accuracy: 1.0000 - val_loss: 3.4660e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 456ms/step - accuracy: 0.9808 - loss: 0.0564 - val_accuracy: 0.9663 - val_loss: 0.0764\n",
            "Epoch 6/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 1.3732e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 459ms/step - accuracy: 0.9795 - loss: 0.0505 - val_accuracy: 0.9769 - val_loss: 0.0586\n",
            "Epoch 8/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0279 - val_accuracy: 1.0000 - val_loss: 4.0618e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 456ms/step - accuracy: 0.9758 - loss: 0.0583 - val_accuracy: 0.9806 - val_loss: 0.0576\n",
            "Epoch 10/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119us/step - accuracy: 0.9688 - loss: 0.0281 - val_accuracy: 1.0000 - val_loss: 4.7206e-05\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 378ms/step - accuracy: 0.9771 - loss: 0.0579\n",
            "Validation Accuracy: 98.13%\n",
            "Epoch 1/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 490ms/step - accuracy: 0.9666 - loss: 0.0823 - val_accuracy: 0.9669 - val_loss: 0.0810\n",
            "Epoch 2/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 110ms/step - accuracy: 0.9375 - loss: 0.1783 - val_accuracy: 1.0000 - val_loss: 0.0072\n",
            "Epoch 3/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 466ms/step - accuracy: 0.9772 - loss: 0.0548 - val_accuracy: 0.9731 - val_loss: 0.0729\n",
            "Epoch 4/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110us/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 1.0000 - val_loss: 6.7949e-06\n",
            "Epoch 5/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 464ms/step - accuracy: 0.9830 - loss: 0.0446 - val_accuracy: 0.9694 - val_loss: 0.0762\n",
            "Epoch 6/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0432 - val_accuracy: 1.0000 - val_loss: 4.7684e-07\n",
            "Epoch 7/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 464ms/step - accuracy: 0.9844 - loss: 0.0366 - val_accuracy: 0.9762 - val_loss: 0.0631\n",
            "Epoch 8/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115us/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 0.0000e+00 - val_loss: 2.1446\n",
            "Epoch 9/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 462ms/step - accuracy: 0.9887 - loss: 0.0283 - val_accuracy: 0.9719 - val_loss: 0.0693\n",
            "Epoch 10/10\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0421 - val_accuracy: 1.0000 - val_loss: 0.0708\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 361ms/step - accuracy: 0.9745 - loss: 0.0685\n",
            "Validation Accuracy after fine-tuning: 97.19%\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Решение\n",
        "#\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "!wget https://storage.yandexcloud.net/academy.ai/cat-and-dog.zip\n",
        "\n",
        "# Разархивируем датасет во временную папку 'temp'\n",
        "!unzip -qo \"cat-and-dog\" -d ./temp\n",
        "\n",
        "# Папка с папками картинок, рассортированных по категориям\n",
        "IMAGE_PATH = './temp/training_set/training_set/'\n",
        "\n",
        "# Папка в которой будем создавать выборки\n",
        "BASE_DIR = './dataset/'\n",
        "\n",
        "# Создаем папки для обучающей и валидационной выборок\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Создаем генераторы данных с аугментацией\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # 20% данных будет использовано для валидации\n",
        ")\n",
        "\n",
        "# Генератор для обучающей выборки\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    IMAGE_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Генератор для валидационной выборки\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    IMAGE_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Загружаем предобученную модель MobileNet\n",
        "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Добавляем GlobalAveragePooling2D и Dense слои\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "# Создаем модель\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Замораживаем слои базовой модели\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Обучаем модель\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Оцениваем модель на валидационной выборке\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Если точность низкая, то попробуем разморозить некоторые слои базовой модели\n",
        "# и продолжитьс более низкой скоростью обучения\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Оцениваем модель на валидационной выборке после тонкой настройки\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Validation Accuracy after fine-tuning: {accuracy * 100:.2f}%')"
      ]
    }
  ]
}